{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1347e528-16ca-4701-9433-324bab9f947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "shots_clean_all_types = pd.read_csv(\"shots_clean_all_types.csv\")\n",
    "shots_comb_zones_all_types = pd.read_csv(\"shots_comb_zones_all_types.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa3272d-c0e7-4199-8cb9-948eaefbd4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== shots_clean_all_types ===\n",
      "['PLAYER_NAME', 'TEAM_NAME', 'PERIOD', 'ACTION_TYPE', 'SHOT_ZONE_BASIC', 'SHOT_ZONE_AREA', 'SHOT_ZONE_RANGE', 'SHOT_DISTANCE', 'SHOT_MADE_FLAG', 'SHOT_VALUE', 'TIME_LEFT_SEC']\n",
      "\n",
      "=== shots_comb_zones_all_types ===\n",
      "['PLAYER_NAME', 'TEAM_NAME', 'PERIOD', 'ACTION_TYPE', 'SHOT_DISTANCE', 'SHOT_MADE_FLAG', 'SHOT_VALUE', 'TIME_LEFT_SEC', 'SHOT_ZONE']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== shots_clean_all_types ===\")\n",
    "print(list(shots_clean_all_types.columns))\n",
    "print()\n",
    "\n",
    "print(\"=== shots_comb_zones_all_types ===\")\n",
    "print(list(shots_comb_zones_all_types.columns))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f4b9e8-3bab-4935-b936-b7f7bdeeaef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training and tuning clean_all dataset =====\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shickey\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:13:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'xgb__subsample': 0.7, 'xgb__n_estimators': 800, 'xgb__max_depth': 7, 'xgb__learning_rate': 0.05, 'xgb__colsample_bytree': 0.7}\n",
      "clean_all accuracy: 0.653 | F1: 0.555\n",
      "\n",
      "===== Training and tuning comb_all dataset =====\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shickey\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:15:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'xgb__subsample': 0.8, 'xgb__n_estimators': 500, 'xgb__max_depth': 10, 'xgb__learning_rate': 0.05, 'xgb__colsample_bytree': 0.5}\n",
      "comb_all accuracy: 0.652 | F1: 0.553\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------\n",
    "# DATASETS\n",
    "# --------------------------\n",
    "datasets = {\n",
    "    \"clean_all\": shots_clean_all_types,\n",
    "    \"comb_all\": shots_comb_zones_all_types,\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# FEATURE SETS\n",
    "# --------------------------\n",
    "feature_sets = {\n",
    "    \"clean_all\": [\n",
    "        \"PLAYER_NAME\", \"TEAM_NAME\",\n",
    "        \"PERIOD\", \"TIME_LEFT_SEC\",\n",
    "        \"ACTION_TYPE\",\n",
    "        \"SHOT_ZONE_BASIC\", \"SHOT_ZONE_AREA\", \"SHOT_ZONE_RANGE\",\n",
    "        \"SHOT_DISTANCE\"\n",
    "    ],\n",
    "    \"comb_all\": [\n",
    "        \"PLAYER_NAME\", \"TEAM_NAME\", \"PERIOD\",\n",
    "        \"ACTION_TYPE\",\n",
    "        \"SHOT_ZONE\",\n",
    "        \"SHOT_DISTANCE\", \"TIME_LEFT_SEC\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "target = \"SHOT_MADE_FLAG\"\n",
    "\n",
    "# --------------------------\n",
    "# HYPERPARAMETER GRID\n",
    "# --------------------------\n",
    "param_grid = {\n",
    "    \"xgb__n_estimators\": [50, 100, 200, 300, 400, 500, 600, 700, 800, 1000],\n",
    "    \"xgb__max_depth\": [2, 3, 4, 5, 6, 7, 8, 10, 12, 15, 20],\n",
    "    \"xgb__learning_rate\": [0.01, 0.02, 0.03, 0.05, 0.07, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"xgb__subsample\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# TRAIN + TUNE FUNCTION\n",
    "# --------------------------\n",
    "def train_xgb_tuned(df, features, target):\n",
    "    X = df[features].copy()\n",
    "    y = df[target]\n",
    "\n",
    "    # detect categoricals\n",
    "    cat_features = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    # base pipeline\n",
    "    model = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # F1 scorer\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "    # Randomized search\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # number of random combinations to try\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    acc = best_model.score(X_test, y_test)\n",
    "    f1 = f1_score(y_test, (y_proba > 0.5).astype(int))\n",
    "    \n",
    "    print(\"Best hyperparameters:\", search.best_params_)\n",
    "    \n",
    "    return best_model, X_test, y_test, y_proba, acc, f1\n",
    "\n",
    "# --------------------------\n",
    "# TRAIN + TUNE ALL DATASETS\n",
    "# --------------------------\n",
    "results_tuned = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n===== Training and tuning {name} dataset =====\")\n",
    "    feats = feature_sets[name]\n",
    "\n",
    "    model, X_test, y_test, y_proba, acc, f1 = train_xgb_tuned(df, feats, target)\n",
    "\n",
    "    results_tuned[name] = {\n",
    "        \"model\": model,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"pred_proba\": y_proba,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "    print(f\"{name} accuracy: {acc:.3f} | F1: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bdb9e5-3c9f-476a-a9dc-4c5797668ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# achieved ideal hyper parameters, since these are nearly identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21cfd50-8452-427c-87dd-96fca1caf88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training and tuning clean_all_reduced dataset =====\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shickey\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:23:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'xgb__subsample': 0.5, 'xgb__n_estimators': 800, 'xgb__max_depth': 8, 'xgb__learning_rate': 0.07, 'xgb__colsample_bytree': 0.7}\n",
      "clean_all_reduced accuracy: 0.651 | F1: 0.554\n",
      "\n",
      "===== Training and tuning comb_all_reduced dataset =====\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shickey\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [12:24:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'xgb__subsample': 0.5, 'xgb__n_estimators': 800, 'xgb__max_depth': 8, 'xgb__learning_rate': 0.07, 'xgb__colsample_bytree': 0.7}\n",
      "comb_all_reduced accuracy: 0.649 | F1: 0.553\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# --------------------------\n",
    "# DATASETS\n",
    "# --------------------------\n",
    "datasets = {\n",
    "    \"clean_all_reduced\": shots_clean_all_types,\n",
    "    \"comb_all_reduced\": shots_comb_zones_all_types,\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# FEATURE SETS\n",
    "# --------------------------\n",
    "feature_sets = {\n",
    "    \"clean_all_reduced\": [\n",
    "        \"PLAYER_NAME\", \"TEAM_NAME\",\n",
    "        \"ACTION_TYPE\",\n",
    "        \"SHOT_ZONE_BASIC\", \"SHOT_ZONE_AREA\", \"SHOT_ZONE_RANGE\",\n",
    "    ],\n",
    "    \"comb_all_reduced\": [\n",
    "        \"PLAYER_NAME\", \"TEAM_NAME\", \n",
    "        \"ACTION_TYPE\",\n",
    "        \"SHOT_ZONE\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "target = \"SHOT_MADE_FLAG\"\n",
    "\n",
    "# --------------------------\n",
    "# HYPERPARAMETER GRID\n",
    "# --------------------------\n",
    "param_grid = {\n",
    "    \"xgb__n_estimators\": [50, 100, 200, 300, 400, 500, 600, 700, 800, 1000],\n",
    "    \"xgb__max_depth\": [2, 3, 4, 5, 6, 7, 8, 10, 12, 15, 20],\n",
    "    \"xgb__learning_rate\": [0.01, 0.02, 0.03, 0.05, 0.07, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"xgb__subsample\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# TRAIN + TUNE FUNCTION\n",
    "# --------------------------\n",
    "def train_xgb_tuned(df, features, target):\n",
    "    X = df[features].copy()\n",
    "    y = df[target]\n",
    "\n",
    "    # detect categorical features\n",
    "    cat_features = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    # pipeline\n",
    "    model = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            eval_metric=\"logloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # F1 scorer\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # number of random combinations\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    acc = best_model.score(X_test, y_test)\n",
    "    f1 = f1_score(y_test, (y_proba > 0.5).astype(int))\n",
    "\n",
    "    print(\"Best hyperparameters:\", search.best_params_)\n",
    "\n",
    "    return best_model, X_test, y_test, y_proba, acc, f1\n",
    "\n",
    "# --------------------------\n",
    "# TRAIN + TUNE ALL REDUCED FEATURE SETS\n",
    "# --------------------------\n",
    "results_reduced = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n===== Training and tuning {name} dataset =====\")\n",
    "    feats = feature_sets[name]\n",
    "\n",
    "    model, X_test, y_test, y_proba, acc, f1 = train_xgb_tuned(df, feats, target)\n",
    "\n",
    "    results_reduced[name] = {\n",
    "        \"model\": model,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"pred_proba\": y_proba,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "    print(f\"{name} accuracy: {acc:.3f} | F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23d6c7af-9fcf-42ac-93e5-99f2b5fd25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced feature sets don't improve accuracy when trained on league wide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b837d04f-c6eb-4a20-b84f-366bca28345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ TEAM-LEVEL PERFORMANCE ================\n",
      "\n",
      "===== clean_all Dataset =====\n",
      "\n",
      "Top Teams by Accuracy:\n",
      "            TEAM_NAME  Accuracy       F1\n",
      "   Washington Wizards  0.742969 0.677134\n",
      "   Philadelphia 76ers  0.699071 0.616226\n",
      "Golden State Warriors  0.686022 0.633779\n",
      "      Milwaukee Bucks  0.678516 0.627240\n",
      "   Los Angeles Lakers  0.677545 0.544457\n",
      "\n",
      "Top Teams by F1:\n",
      "            TEAM_NAME  Accuracy       F1\n",
      "   Washington Wizards  0.742969 0.677134\n",
      "Golden State Warriors  0.686022 0.633779\n",
      "      Milwaukee Bucks  0.678516 0.627240\n",
      "   Philadelphia 76ers  0.699071 0.616226\n",
      "     Sacramento Kings  0.664454 0.599119\n",
      "\n",
      "===== comb_all Dataset =====\n",
      "\n",
      "Top Teams by Accuracy:\n",
      "            TEAM_NAME  Accuracy       F1\n",
      "   Washington Wizards  0.738005 0.697183\n",
      "   Philadelphia 76ers  0.706833 0.606120\n",
      "   Los Angeles Lakers  0.678274 0.517594\n",
      "         Phoenix Suns  0.677468 0.543710\n",
      "Golden State Warriors  0.674174 0.638333\n",
      "\n",
      "Top Teams by F1:\n",
      "             TEAM_NAME  Accuracy       F1\n",
      "    Washington Wizards  0.738005 0.697183\n",
      " Golden State Warriors  0.674174 0.638333\n",
      "    Philadelphia 76ers  0.706833 0.606120\n",
      "       Milwaukee Bucks  0.663853 0.600365\n",
      "Minnesota Timberwolves  0.656200 0.585839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n================ TEAM-LEVEL PERFORMANCE ================\")\n",
    "\n",
    "for dataset_name, res in results_tuned.items():\n",
    "    print(f\"\\n===== {dataset_name} Dataset =====\")\n",
    "\n",
    "    model  = res[\"model\"]\n",
    "    X_test = res[\"X_test\"].copy()\n",
    "    y_test = res[\"y_test\"]\n",
    "\n",
    "    # Ensure TEAM_NAME is available for grouping\n",
    "    if \"TEAM_NAME\" not in X_test.columns:\n",
    "        print(\"TEAM_NAME missing — cannot compute team stats.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Recompute predictions using the stored model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Build dataframe for team-level analysis\n",
    "    df_team = pd.DataFrame({\n",
    "        \"TEAM_NAME\": X_test[\"TEAM_NAME\"],\n",
    "        \"y_true\": y_test,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    team_stats = []\n",
    "    for team, group in df_team.groupby(\"TEAM_NAME\"):\n",
    "        acc = accuracy_score(group[\"y_true\"], group[\"y_pred\"])\n",
    "        f1  = f1_score(group[\"y_true\"], group[\"y_pred\"], zero_division=0)\n",
    "        team_stats.append((team, acc, f1))\n",
    "\n",
    "    team_df = pd.DataFrame(team_stats, columns=[\"TEAM_NAME\", \"Accuracy\", \"F1\"])\n",
    "\n",
    "    # Top 5 teams by each metric\n",
    "    top_acc = team_df.sort_values(\"Accuracy\", ascending=False).head(5)\n",
    "    top_f1  = team_df.sort_values(\"F1\", ascending=False).head(5)\n",
    "\n",
    "    print(\"\\nTop Teams by Accuracy:\")\n",
    "    print(top_acc.to_string(index=False))\n",
    "\n",
    "    print(\"\\nTop Teams by F1:\")\n",
    "    print(top_f1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0f7bc9-fe74-42b2-b73d-f3c1ebcc10c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ TEAM-LEVEL PERFORMANCE ================\n",
      "\n",
      "===== clean_all_reduced Dataset =====\n",
      "\n",
      "Top Teams by Accuracy:\n",
      "            TEAM_NAME  Accuracy       F1\n",
      "   Washington Wizards  0.739844 0.671273\n",
      "   Philadelphia 76ers  0.705504 0.619926\n",
      "Golden State Warriors  0.693907 0.639053\n",
      "     Sacramento Kings  0.679941 0.614565\n",
      "   Los Angeles Lakers  0.679099 0.533333\n",
      "\n",
      "Top Teams by F1:\n",
      "            TEAM_NAME  Accuracy       F1\n",
      "   Washington Wizards  0.739844 0.671273\n",
      "Golden State Warriors  0.693907 0.639053\n",
      "   Philadelphia 76ers  0.705504 0.619926\n",
      "      Milwaukee Bucks  0.672334 0.618018\n",
      "     Sacramento Kings  0.679941 0.614565\n",
      "\n",
      "===== comb_all_reduced Dataset =====\n",
      "\n",
      "Top Teams by Accuracy:\n",
      "            TEAM_NAME  Accuracy       F1\n",
      "   Washington Wizards  0.738766 0.694023\n",
      "   Philadelphia 76ers  0.700955 0.604470\n",
      "   Los Angeles Lakers  0.691900 0.543210\n",
      " New Orleans Pelicans  0.678214 0.582834\n",
      "Golden State Warriors  0.670420 0.633250\n",
      "\n",
      "Top Teams by F1:\n",
      "            TEAM_NAME  Accuracy       F1\n",
      "   Washington Wizards  0.738766 0.694023\n",
      "Golden State Warriors  0.670420 0.633250\n",
      "      Milwaukee Bucks  0.666922 0.605455\n",
      "   Philadelphia 76ers  0.700955 0.604470\n",
      "    San Antonio Spurs  0.638179 0.598048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n================ TEAM-LEVEL PERFORMANCE ================\")\n",
    "\n",
    "for dataset_name, res in results_reduced.items():\n",
    "    print(f\"\\n===== {dataset_name} Dataset =====\")\n",
    "\n",
    "    model  = res[\"model\"]\n",
    "    X_test = res[\"X_test\"].copy()\n",
    "    y_test = res[\"y_test\"]\n",
    "\n",
    "    # Ensure TEAM_NAME is available for grouping\n",
    "    if \"TEAM_NAME\" not in X_test.columns:\n",
    "        print(\"TEAM_NAME missing — cannot compute team stats.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Recompute predictions using the stored model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Build dataframe for team-level analysis\n",
    "    df_team = pd.DataFrame({\n",
    "        \"TEAM_NAME\": X_test[\"TEAM_NAME\"],\n",
    "        \"y_true\": y_test,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    team_stats = []\n",
    "    for team, group in df_team.groupby(\"TEAM_NAME\"):\n",
    "        acc = accuracy_score(group[\"y_true\"], group[\"y_pred\"])\n",
    "        f1  = f1_score(group[\"y_true\"], group[\"y_pred\"], zero_division=0)\n",
    "        team_stats.append((team, acc, f1))\n",
    "\n",
    "    team_df = pd.DataFrame(team_stats, columns=[\"TEAM_NAME\", \"Accuracy\", \"F1\"])\n",
    "\n",
    "    # Top 5 teams by each metric\n",
    "    top_acc = team_df.sort_values(\"Accuracy\", ascending=False).head(5)\n",
    "    top_f1  = team_df.sort_values(\"F1\", ascending=False).head(5)\n",
    "\n",
    "    print(\"\\nTop Teams by Accuracy:\")\n",
    "    print(top_acc.to_string(index=False))\n",
    "\n",
    "    print(\"\\nTop Teams by F1:\")\n",
    "    print(top_f1.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee1247-bcff-49ff-894f-25abe475c44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ffa5ae5-d6fa-454c-a3f1-f160f216ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing features improves accuracy when tested on a specific team, although the score is still lower than original because its not trained on all teams data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02f55d-f536-44e0-8f18-0f707540e863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a32f1-09e3-4f2a-a356-281a295c7b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb60950-b0da-4796-8666-048ee031c38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6da50a-ae96-456a-839b-c0da64a2e148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ae665-ed28-4b2e-8dfc-8d4645709ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
